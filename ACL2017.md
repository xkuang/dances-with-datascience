# ACL 2017 newbie takeaway

### Quick summary
Computational Linguistics  -> Computer Science + Linguistics. Technical(computer science)+ humanitarian(linguistics).

Very developed area -  55th yearly conference .

Emerging from academic into industrial sector.
1900 attendees this year 30% from industrial  and  70% academic (doubled comparing to the last year)

Areas:  semantics, machine translation, speech generation, computer vision, dialogue, questions and answers  disclosure, syntax, morphology and so on.

Tasks: text simplification , movie summarization, robot-helper. 

### General resources

http://web.stanford.edu/class/cs224n/  Natural Language Processing with Deep Learning  <br />
http://www.wildml.com blog  <br />
https://www.youtube.com/watch?v=nfoudtpBV68 NLP course by Dan Jurafsky


## Models

### Sequence to sequence model (seq2seq)  

#### How it works: 
2RNNs encoder and decoder, intermediate representation in the middle.

#### Main idea: 
from source sentence predict target sentence.

#### Where it’s used: 
Dialogue Systems and Machine Translation

#### Resources: 
https://www.youtube.com/watch?v=RIR_-Xlbp7s  <br />
https://www.tensorflow.org/tutorials/seq2seq  <br />
http://suriyadeepan.github.io/2016-12-31-practical-seq2seq/  <br />
https://arxiv.org/pdf/1406.1078.pdf


### Bidirectional LSTM (BLSTM)

#### How it works: 
It involves duplicating the first recurrent layer in the network so that there are now two layers side-by-side, then providing the input sequence as-is as input to the first layer and providing a reversed copy of the input sequence to the second.

#### Main idea: 
use information from both  past and future to predict current word.

#### Where it’s used: 
handwriting recognition, speech recognition, translation

#### Resources:
https://machinelearningmastery.com/develop-bidirectional-lstm-sequence-classification-python-keras/  <br />
https://stackoverflow.com/questions/43035827/whats-the-difference-between-a-bidirectional-lstm-and-an-lstm  <br />
https://en.wikipedia.org/wiki/Bidirectional_recurrent_neural_networks

### Attention based RNN:

#### How it works:
With an attention mechanism we no longer try encode the full source sentence into a fixed-length vector. Rather, we allow the decoder to “attend” to different parts of the source sentence at each step of the output generation. Importantly, we let the model learn what to attend to based on the input sentence and what it has produced so far. So, in languages that are pretty well aligned (like English and German) the decoder would probably choose to attend to things sequentially. Attending to the first word when producing the first English word, and so on. The important part is that each decoder output word now depends on a weighted combination of all the input states, not just the last state.

#### Main idea:
helps to solve long-range dependencies issue with seq2seq

#### Where it’s used:
machine translation, image caption

#### Resources:
https://arxiv.org/pdf/1502.03044.pdf  <br />
http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/


### Pyramid CNN:
https://arxiv.org/pdf/1608.04064.pdf


### Hierarchical Multiscale LSTM(HMLSTM) -  character by character + word by word

https://arxiv.org/pdf/1609.01704.pdf  <br />
https://github.com/n-s-f/hierarchical-rnn



## Learning:

### Multi-View Representation Learning: 

Multi-view data : audio+video, audio+articulation, video+fMRI, image+text, webpage+click-through data, and text in different languages; or may consist of synthetic views of the same measurements, such as different time steps of a time sequence, word+context words, ordifferent parts of a parse tree

### DL2R (Deep learning to respond)

To establish an automatic conversation system between humans and computers  <br />
https://www.semanticscholar.org/paper/Learning-to-Respond-with-Deep-Neural-Networks-for-Yan-Song/7477d88b225909ef645941a0142eed75dc3b2e56  <br />
http://www.ruiyan.me/pubs/SIGIR2016.pdf

### Reinforcement learning:

Reinforcement learning is the problem of getting an agent to act in the world so as to maximize its rewards. (Mentioned by Facebook AI)  <br />
http://www.wildml.com/2016/10/learning-reinforcement-learning/  <br />
http://ir.hit.edu.cn/~jguo/docs/notes/dqn-atari.pdf  <br />
https://www.cs.ubc.ca/~murphyk/Bayes/pomdp.html 


## Evaluation methods:

### BLEU
Method for Automatic Evaluation of Machine Translation  <br />
https://stackoverflow.com/questions/32395880/calculate-bleu-score-in-python  <br />
http://www.aclweb.org/anthology/P02-1040.pdf  <br />
The primary programming task for a BLEU implementor is to compare n-grams of the candidate with the n-grams of the reference translation and count the number of matches. These matches are position- independent. The more the matches, the better the candidate translation is.
Scores are calculated for individual translated segments—generally sentences—by comparing them with a set of good quality reference translations.

### SARI
Evaluation metrics for simplified text.   <br />
https://cocoxu.github.io/publications/tacl2016-smt-simplification.pdf

### ADEM
Evaluation metrics for dialogue response  <br />
https://openreview.net/pdf?id=HJ5PIaseg

### ROUGE
Set of metrics for evaluating automatic summarization of texts  <br />
http://www.rxnlp.com/how-rouge-works-for-evaluation-of-summarization-tasks/

### ME TEOR
Metrics for machine translation evaluation improvement of BLEU  <br />
https://www.cs.cmu.edu/~alavie/papers/BanerjeeLavie2005-final.pdf  <br />
http://www.statmt.org/wmt08/pdf/WMT12.pdf

### Perplexity evaluation: 
https://www.quora.com/How-does-perplexity-function-in-natural-language-processing  <br />
https://www.youtube.com/watch?v=OHyVNCvnsTo

## Datasets:

### MS-COCO
image recognition, segmentation, and captioning dataset  <br />
http://cocodataset.org/dataset.htm#overview

### Imsitu
dataset supporting situation recognition, the problem of producing a concise summary of the situation an image depicts  <br /> http://imsitu.org 

### WordNet
WordNet® is a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms  <br /> (synsets), each expressing a distinct concept. https://wordnet.princeton.edu

### FrameNet
FrameNet is a project which produces an electronic resource based on a theory of meaning called frame semantics. FrameNet reveals for example that the sentence "John sold a car to Mary" essentially describes the same basic situation (semantic frame) as "Mary bought a car from John", just from a different perspective. A semantic frame can be thought of as a conceptual structure describing an event, relation, or object and the participants in it. The FrameNet lexical database contains over 1,200 semantic frames, 13,000 lexical units (a pairing of a word with a meaning; polysemous words are represented by several lexical units) and 202,000 example sentences.
https://en.wikipedia.org/wiki/FrameNet

### BabelNet
multilingual resource that covers hundreds of languages and, according to need, can be used as either an encyclopedic dictionary, or a semantic network, or a huge knowledge base  <br />
http://babelnet.org

### TACOS
video description corpus.  <br />
https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/vision-and-language/tacos-multi-level-corpus/


## Interesting people:

Mirella Lapata  <br />
Jason Weston   <br />
Barbara Grosz  <br />
Noah Smith  <br />
Dan Jurafsky  <br />

## Other related things:

### TFIDF
term frequency-inverse document frequency  <br />
http://www.tfidf.com/  <br />

This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus
can be successfully used for stop-words filtering in various subject fields including text summarization and classification.  <br />
https://www.quora.com/How-does-TF-IDF-work

### PCA projection of closer meaning sentences.
https://arxiv.org/pdf/1409.3215.pdf  - seq2seq

### Coreference
sometimes written co-reference, occurs when two or more expressions in a text refer to the same person or thing

## Papers:
Show and Tell: A Neural Image Caption Generator Vinyals(2015)   <br />
Language to Logical Form with Neural Attention Dong and Lapata(2016)  <br />
Data Recombination for Neural Semantic Parsing Jia and Liang(2016)  <br />
Modified LSTM Cell (Wen 2015, 2016)  <br />
Structured prediction (Belanger and Mccallum 2016)

## Something I don't know yet:
D-NTM  <br />
MemN2N  <br />
DNC  <br />
MemNet  <br />
EntNet   <br />
QRN  <br />
Boltzman machine (and restricted)  <br />
