{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting location of a missing word through ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This script goes through using n-grams to predict the location of a missing word in a sentence. The problem is based on a competition previously held on [Kaggle](https://www.kaggle.com/c/billion-word-imputation), to impute a singular word into a sentence . That problem can be solved in two subtasks: predicitng the missing word's location, and then inserting the most probable word. Here we attempt to model the former."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string \n",
    "import nltk\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data can be found (at the time of writing) on the Kaggle competition page for [download](https://www.kaggle.com/c/billion-word-imputation/download/train_v2.txt.zip). The training data are approximately 4.15Gb after unzipping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/train/train_v2.txt\", 'r') as f:\n",
    "    #f.readline() # skip header\n",
    "    corpus = f.readlines()\n",
    "# check cleanliness if need be\n",
    "len(corpus)\n",
    "# 30301028"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data come to us as complete sentences, with no particular ordering or themes, but thankfuly (presumably) cleaned and ready to work with.\n",
    "For our proof of concept we will do just the necessary preprocessing to work with our sentences - worrying about more thourghough work at a later date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create our training set with a randomly removed word from each sentence, noting the word and it's location in the sentence. For the rules of this competition, the first and last word of a sentence could not be removed, BUT the last 'word' was always \".\", which we strip out anyways with the rest of punctuation. For proof of concept and computing time, we will downsample to only the first 5000 sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pullword(l):\n",
    "    \"\"\" Removes a random item from a tokenized list\"\"\"\n",
    "    temp = l\n",
    "    index = temp.index(random.Random(0).choice(temp[1:]))\n",
    "    y_train.append(temp[index])\n",
    "    y_train_index.append(index)\n",
    "\n",
    "    temp.pop(index)\n",
    "    X_train.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exclude = set(string.punctuation)\n",
    "train = [nltk.word_tokenize(\"\".join(ch for ch in line.strip(\"\\n\") if ch not in exclude)) for line in corpus[0:5000] ]   # sampledown train for proof of concept\n",
    "X_train, y_train, y_train_index = [], [], []\n",
    "for line in train[0:5000]: \n",
    "    if len(line) <= 2:\n",
    "        train.pop(train.index(line))\n",
    "    else:\n",
    "        pullword(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good. We now have 4 key data structures to run through our model: train, X_train (with a removed word), y_train (the removed word), and y_train_index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model\n",
    "### Predicting where a word is missing from a sentence can be done in multiple ways, including Parts of Speech (described elsewhere in this repo), and n-gram probability which we do here. \n",
    "\n",
    "### Given the number of occurences of all bigrams C(w1,w2) and the occurences of all trigrams C(w,1,wx,w2), we calculate the number of occurences, D(w1,w,w2), where the is one and only one word inbetween w1 and w3. We can then apply this as a probability in our word-removed sentences, scoring which bigram is the most likely to actually be a trigram of the form D(w1,w,w2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO format formulae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this we must first set up our bigrams/trigrams, saving the output to persistence should we later revisit the model. This will become important once running a fuller data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_bigram = CountVectorizer(tokenizer=lambda doc: doc, \n",
    "                               analyzer='word', \n",
    "                               input=u'content', \n",
    "                               ngram_range=(2,2), \n",
    "                               min_df=0.0, \n",
    "                               lowercase=False).fit(train)\n",
    "bigrams = cv_bigram.transform(train)\n",
    "joblib.dump(bigrams, \"persistence/bigrams.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_trigram = CountVectorizer(tokenizer=lambda doc: doc, \n",
    "                               analyzer='word', \n",
    "                               input=u'content', \n",
    "                               ngram_range=(3,3), \n",
    "                               min_df=0.0, \n",
    "                               lowercase=False).fit(train)\n",
    "trigrams = cv_trigram.transform(train)\n",
    "joblib.dump(bigrams, \"persistence/trigrams.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Occurence counts can be seen quite quickly, but the sparse array implemented by scipy is not particularly useful for inspection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sum the occurences of C(w1, w2) bigrams\n",
    "Cbigram = bigrams.sum(axis=0)\n",
    "# sum the occurnces of C(w1, w2, w3) trigrams\n",
    "Ctrigram = trigrams.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create D(w1,w2) , the number of occurences of the trigram of the form w1, w, w3 to use as out probability of a bigram actually being a trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bgs = []\n",
    "for item in cv_bigram.vocabulary_.items():\n",
    "    l = item[0].split()\n",
    "    words = (l[0], l[1])\n",
    "    bgs.append(words)\n",
    "      \n",
    "tgs = []\n",
    "for item in cv_trigram.vocabulary_.items():\n",
    "    l = item[0].split()\n",
    "    words = (l[0], l[1], l[2])\n",
    "    tgs.append(words)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is the largest step of creating our model, and takes a long time (many hours), even on the small proof-of-concept subsample. It is highly recommended to pickle the output once run, so it does not need to be recreated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# form a dictionary of bigrams that could be the outer trigram\n",
    "E = dict() # for the bigram (this is what we'll use )\n",
    "for tg in tgs:\n",
    "    for bg in bgs:\n",
    "        if (tg[0] == bg[0]) and (tg[2] == bg[1]):\n",
    "            bgram = \" \".join(s for s in bg)\n",
    "            tgram = \" \".join(s for s in tg)\n",
    "            index = cv_trigram.vocabulary_.get(\" \".join(s for s in tg))\n",
    "            if bgram not in E.keys():\n",
    "                E[bgram] = [index] \n",
    "            else:\n",
    "                if index not in E[bgram]:\n",
    "                    E[bgram].append(index)\n",
    "                else:\n",
    "                    print(\"index: \" + str(index) + \" is already in E[\" +str(bgram) +\"]\") # something weird has occurred.\n",
    "joblib.dump(E, \"persistence/E.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And create a score matrix for every bigram occurence as a possible trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D = {}\n",
    "for item in cv_bigram.vocabulary_:\n",
    "    D[item] = 0\n",
    "# sum the counts for each bigram in E by looking up in cv_bigrams\n",
    "for item in E:\n",
    "    for index in E[item]:\n",
    "        D[item] += trigrams[:, index].sum()\n",
    "# drop zeros\n",
    "D2 = { k:v for k, v in D.items() if v }\n",
    "joblib.dum(D2, \"persistence/D2.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the location of the missing word is now able to be performed. The idea is, that in each sentence, all bigrams can be compared, and the one with the highest probability of being a trigram is the location of the missing word. Note that this works because in our test, we _know_ that a word is missing, but if we didn't we could set a threshold or other metric instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trigram_probality(bigram_as_list):\n",
    "    \"\"\"Returns the probability of a single bigram to be a trigram, given the trained model.\"\"\"\n",
    "    bg = \" \".join(bigram_as_list)\n",
    "    if cv_bigram.vocabulary_.get(bg):    \n",
    "        bg_index = cv_bigram.vocabulary_.get(bg)\n",
    "        prob = D2[bg] / (bigrams[:, bg_index].sum() + D2[bg])\n",
    "        print(bg, prob) \n",
    "        return D2[bg] / (bigrams[:, bg_index].sum() + D2[bg])\n",
    "    else:\n",
    "        return 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bigrams_formatter(l):\n",
    "    \"\"\"Helper function to ensure the bigrams are passed correctly to trigram_probability.\"\"\"\n",
    "    bg = []\n",
    "    for i in range(len(l)-1):\n",
    "        bg.append( str(l[i] + \" \" + str(l[i+1])))\n",
    "    return bg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_sentence(sentence_as_list):\n",
    "    \"\"\"Creates a list of probability scores for a given sentence, returning the index of the most likely position.\"\"\"\n",
    "    if type(sentence_as_list) == str:\n",
    "        sentence_as_list = sentence_as_list.split()\n",
    "    s = []\n",
    "    for bigram in bigrams_formatter(sentence_as_list):\n",
    "        s.append(trigram_probality(bigram))\n",
    "    print(\"bigram-is-trigram probability: \", s)\n",
    "    # in the case that all are zero, we will pick the first word, to let us know the model is failing\n",
    "    #if s.index(max(s)) == 0:\n",
    "    return s.index(max(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it. we can now test it out a bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gram = \"the of\"\n",
    "sentence = \"The dog chased after the \"\n",
    "score_sentence(gram.split())\n",
    "score_sentence(sentence.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy evaluation\n",
    "### To test our accuracy, we can now use X_train and y_train_index to see how well this model predicts the location of a missing word... sorta! Remember that we used (almost) the same sentences to create our bigrams and trigrams counts, so we're double-dipping into our data. But if we don't get a good result here, then we won't anywhere else either, and will have to re-approach the model as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "guess = []\n",
    "for sentence in X_train:\n",
    "    guess.append(score_sentence(sentence))\n",
    "guess = pd.Series(guess)    \n",
    "missing = pd.Series(y_train_index)\n",
    "\n",
    "result = pd.concat([guess, missing], axis=1)\n",
    "result['hit'] = np.where(result['guess'] == result['missing'], 1, 0)\n",
    "\n",
    "accuracy = result['hit'].sum() / len(result['hit']) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO input results table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "### There you have it. We went through tokenizing a large text dataset, created massive (and sparse) arrays of vectorized counts, and created a bigram-trigram comparison model to predict where in a sentence a word is missing. \n",
    "\n",
    "### The next step, is to impute the word, which again, can be done in a multitude of ways. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
